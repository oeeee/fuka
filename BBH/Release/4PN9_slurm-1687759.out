No dat file associated with : ./initial_bbh_d1411.info
Creating a setup based on config file...
Solutions will be stored in: /home/la34cit/fuka//COs/
Directory will be created if it doesn't exist.
Config File: /home/la34cit/fuka//COs/initbh.info
Fields File: /home/la34cit/fuka//COs/initbh.dat
*****************bh info*****************
                 chi: 0
                 dim: 3
         fixed_lapse: 0.3
                 mch: 0.5
                mirr: 0.5
             nshells: 0
               omega: 0
                qpig: 12.5664
                 res: 9
                 rin: 0.208117
                rmid: 0.416233
                rout: 1.66493
                velx: 0
                vely: 0

Config File: /home/la34cit/fuka//COs/converged_BH_TOTAL_BC.0.5.0.0.09.info
Fields File: /home/la34cit/fuka//COs/converged_BH_TOTAL_BC.0.5.0.0.09.dat
*****************bh info*****************
                 chi: 0
                 dim: 3
         fixed_lapse: 0.434936
                 mch: 0.5
                mirr: 0.5
             nshells: 0
               omega: -6.25718e-17
                qpig: 12.5664
                 res: 9
                 rin: 0.208117
                rmid: 0.42977
                rout: 1.66493
                velx: 0
                vely: 0

Solved previously: /home/la34cit/fuka//COs/converged_BH_TOTAL_BC.0.5.0.0.09.info
############################
Binary boost using von Neumann BC
############################
=======================================
Iter:        +1
Error:       +0
Madm:        +0.500087
Mk:          +0.497314 [+0.00554512]
Mirr:        +0.5
R:           +0.42977
Omega:       -6.25718e-17
S:           +8.61997e-18
Mch:         +0.5
Chi:         +3.44799e-17
=======================================

Error init = 0.00750884, Eq: B^i = N / P^2 * sm^i + ome * mg^i Dom: 2 Bounday: 2
DOF / rank = 10611 x 10611 / 330 = 341192
Block size is 32, consider lowering the number of cores.
--------------------------------------------------------------------------
WARNING: Open MPI failed to TCP connect to a peer MPI process.  This
should not happen.

Your Open MPI job may now hang or fail.

  Local host: node188
  PID:        413016
  Message:    connect() to 192.168.209.248:1024 failed
  Error:      Operation now in progress (115)
--------------------------------------------------------------------------
slurmstepd: error: *** JOB 1687759 ON node167 CANCELLED AT 2022-08-18T15:22:58 ***
